{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotnine as pln\n",
    "import pickle as pkl\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "random_state = np.random.RandomState(37676373)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the file and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./ML case/data.csv', parse_dates=['date_in'], dtype={'agency_rating': np.int64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`agancy_rating`, the only ordinal variable will be treated as a numeric one for the sake of this classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['date_in'].dt.year\n",
    "data['month'] = data['date_in'].dt.month\n",
    "data['day'] = data['date_in'].dt.day\n",
    "data['weekday'] = data['date_in'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['house_pk'].value_counts().plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['agency_id', 'agency_rating']].pivot_table(index='agency_id', columns='agency_rating', aggfunc=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to preserve agency_id for now\n",
    "data = pd.concat([data['agency_id'], pd.get_dummies(data, columns=['agency_id'], drop_first=True)], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data anlysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will discover the relationship between the target variable and the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot y's (price's) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the distribution of price is highly non-normal, it will be more reasonable to work with log-normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price'] = np.log10(data['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mean price over time and add some context to it with all of the observations on one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.DataFrame({'start': ['2016-07-01', '2017-07-01', '2018-07-01'], 'finish': ['2016-08-31', '2017-08-31', '2018-08-31'], \n",
    "                         'ymin': [-np.inf, -np.inf, -np.inf], 'ymax': [np.inf, np.inf, np.inf]}, )\n",
    "holidays['start'] = pd.to_datetime(holidays['start'])\n",
    "holidays['finish'] = pd.to_datetime(holidays['finish'])\n",
    "\n",
    "christmas = pd.DataFrame({'start': ['2016-12-20', '2017-12-20', '2018-12-20'], 'finish': ['2017-01-15', '2018-01-15', '2019-01-15'], \n",
    "                         'ymin': [-np.inf, -np.inf, -np.inf], 'ymax': [np.inf, np.inf, np.inf]}, )\n",
    "christmas['start'] = pd.to_datetime(christmas['start'])\n",
    "christmas['finish'] = pd.to_datetime(christmas['finish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pln.ggplot(pln.aes('date_in', 'price', group='factor(house_pk)'), data=data)\n",
    " + pln.geom_line(color='gray', alpha=0.3, size=0.5)\n",
    " + pln.geom_line(pln.aes('date_in', 'price'), data.groupby('date_in')['price'].agg('mean').to_frame().reset_index(drop=False), size=3, inherit_aes=False)\n",
    " + pln.geom_rect(pln.aes(xmin='start', xmax='finish', ymin='ymin', ymax='ymax'), holidays, fill='green', alpha=0.25, inherit_aes=False)\n",
    " + pln.geom_rect(pln.aes(xmin='start', xmax='finish', ymin='ymin', ymax='ymax'), christmas, fill='red', alpha=0.25, inherit_aes=False)\n",
    " + pln.labels.xlab('Date')\n",
    " + pln.labels.ylab('Price')\n",
    " + pln.theme_bw()\n",
    " + pln.theme(axis_text_x=pln.element_text(rotation=90, hjust=0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It pays off to plot the distribution with respect to the year even if it cannot be used as an explanatory variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_variables = ['agency_id', 'apartment', 'indoor_pool', 'spa', 'internet', 'pets_allowed', 'water_view', 'fire_stove', 'agency_rating', 'year', \n",
    "                 'month', 'weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = len(cat_variables) // 2 + len(cat_variables) % 2\n",
    "n_cols = 2\n",
    "fig, ax = plt.subplots(ncols=n_cols, nrows=n_rows, sharey=True, figsize=(18, 50))\n",
    "\n",
    "for i, var in enumerate(cat_variables):\n",
    "    var_name = var.upper().replace('_', ' ')\n",
    "    sns.boxplot(data=data, x=var, y='price', ax=ax[i // 2, i % 2])\n",
    "    # please mind that the data has been already log-transformed\n",
    "    _, p = stats.f_oneway(*[data['price'].loc[x[1][var].index].values for x in data[var].to_frame().groupby(var, squeeze=True)])\n",
    "    ax[i // 2, i % 2].set_title(f'{var_name}, one-way ANOVA p-value: {np.round(p, 4)}')\n",
    "    ax[i // 2, i % 2].set_xlabel(var_name)\n",
    "    ax[i // 2, i % 2].set_ylabel('PRICE')\n",
    "    \n",
    "# remove unused axes\n",
    "if i+1 != n_rows*n_cols:\n",
    "    ax.flat[-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_variables = ['dis_water_real', 'dis_shopping', 'no_bedrooms', 'max_persons', 'house_size', 'land_size', 'build_year', 'renovation_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, `no_bedrooms` could be treated as a categorical variable due to the low number of values it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = len(con_variables) // 2 + len(con_variables) % 2\n",
    "n_cols = 2\n",
    "\n",
    "fig, ax = plt.subplots(ncols=n_cols, nrows=n_rows, sharey=True, figsize=(18, 30))\n",
    "\n",
    "for i, var in enumerate(con_variables):\n",
    "    var_name = var.upper().replace('_', ' ')\n",
    "    sns.scatterplot(data=data, x=var, y='price', ax=ax[i // 2, i % 2])\n",
    "    corr, p_value = stats.pearsonr(data['price'], data[var])\n",
    "    ax[i // 2, i % 2].set_title(f'{var_name}, correlation: {np.round(corr, 5)}, non-correlation test p-value: {np.round(p_value, 5)}')\n",
    "    ax[i // 2, i % 2].set_xlabel(var_name)\n",
    "    ax[i // 2, i % 2].set_ylabel('PRICE')\n",
    "    \n",
    "# remove unused axes\n",
    "if i+1 != n_rows*n_cols:\n",
    "    ax.flat[-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables = cat_variables + con_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.xkcd_palette(colors=[\"windows blue\", \"amber\"])\n",
    "col_mapping = [palette[0] if var in cat_variables else palette[1] for var in all_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend for the rectangles on the plot\n",
    "sns.palplot(palette)\n",
    "for i, val in enumerate(['categorical', 'continuous']):\n",
    "    plt.text(i, 0, val, horizontalalignment='center', verticalalignment='center', rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(data[all_variables].corr(method='pearson'), method='ward', figsize=(15, 15), row_cluster=True, col_cluster=True, col_colors=col_mapping,\n",
    "               row_colors=col_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression model, solely on tabular data, to predict price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no weekday\n",
    "X_features = ['dis_water_real', 'dis_shopping', 'no_bedrooms', 'max_persons', 'house_size', 'land_size', 'build_year', 'renovation_year', 'apartment', \n",
    "              'indoor_pool', 'spa', 'internet', 'pets_allowed', 'water_view', 'fire_stove', 'agency_rating', 'year', 'month', 'day', 'agency_id_121', \n",
    "              'agency_id_130', 'agency_id_160']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set consists of 15% of observations for a given house_pk within a given year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = data.groupby(['house_pk', 'year', 'month']).apply(lambda gr: gr.sample(frac=0.15, random_state=random_state)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.merge(data, validation_set, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert validation_set.shape[0] > 0 and training_set.shape[0] > 0 and validation_set.shape[0] + training_set.shape[0] == data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=250, criterion='mse', max_features='sqrt', n_jobs=3, random_state=random_state)\n",
    "\n",
    "rf_grid = {'max_depth': [20, 22, 25, 27, 30]}  # the grid was chosen after some initial trials\n",
    "\n",
    "rf_cv = GridSearchCV(rf_model, rf_grid, scoring='neg_mean_squared_error', \n",
    "                     cv=RepeatedKFold(n_splits=5, n_repeats=3, random_state=random_state), \n",
    "                     n_jobs=2, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the model takes considerable amount of time, so it has been saved and is ready to be re-used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_cv.fit(training_set[X_features], training_set['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(rf_cv, open('rf_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = pkl.load(open('rf_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf_cv.predict(validation_set[X_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = lgbm.LGBMRegressor(n_jobs=2, num_leaves=2**10, random_state=37676373, importance_type='gain', )\n",
    "\n",
    "lgbm_grid = {'n_estimators': Integer(50, 500),\n",
    "             'learning_rate': Real(0.01, 0.35, 'uniform'),\n",
    "             'reg_alpha': Real(1e-6, 1, 'log-uniform'), \n",
    "             'reg_lambda': Real(1e-6, 1, 'log-uniform'), \n",
    "             'gamma': Real(0, 1, 'uniform'), \n",
    "             'subsample': Real(0.5, 1, 'uniform'), \n",
    "             'colsample_bytree': Real(0.5, 1, 'uniform'),\n",
    "             'max_depth': Integer(10, 30)\n",
    "            }\n",
    "\n",
    "lgbm_cv = BayesSearchCV(lgbm_model, lgbm_grid, n_iter=25, scoring='neg_mean_squared_error', refit=True, n_points=1,\n",
    "                        cv=RepeatedKFold(n_splits=5, n_repeats=3, random_state=random_state),\n",
    "                        n_jobs=3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_cv.fit(training_set[X_features], training_set['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(lgbm_cv, open('lgbm_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_cv = pkl.load(open('lgbm_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_predictions = lgbm_cv.predict(validation_set[X_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare  both models side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, True, True, figsize=(20, 7))\n",
    "# the two lines below are for common labels for both plots\n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "\n",
    "ax[0].plot(validation_set['price'], rf_predictions, color='green', marker='x', linewidth=0)\n",
    "ax[0].set_title(f'Random forest predictions - MSE: {np.round(mean_squared_error(validation_set[\"price\"], rf_predictions), 5)}',\n",
    "                fontdict={'fontsize': 20, 'fontweight': 'bold'})\n",
    "\n",
    "ax[1].plot(validation_set['price'], lgbm_predictions, color='blue', marker='x', linewidth=0)\n",
    "ax[1].set_title(f'LightGBM predictions - MSE: {np.round(mean_squared_error(validation_set[\"price\"], lgbm_predictions), 5)}',\n",
    "                fontdict={'fontsize': 20, 'fontweight': 'bold'})\n",
    "\n",
    "plt.xlabel('log10(Real price)')\n",
    "plt.ylabel('log10(Predicted price)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'importance': lgbm_cv.best_estimator_.feature_importances_, 'features': X_features}).sort_values('importance', ascending=False).\\\n",
    "    plot(x='features', y='importance', kind='bar', figsize=(12, 9), title='Feature importance for LGBM model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spa'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's add images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wand.image import Image as w_Image\n",
    "from wand.display import display\n",
    "import matplotlib.image as matimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_imgs_folder = pathlib.Path('./ML case/aerial_photos/complete_photos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief look at some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some pictures are identical\n",
    "sample_of_house_pk = ['7602', '7603', '7604', '7605', '27735', '27742', '27743']\n",
    "sample_pictures = [x + '.png' for x in sample_of_house_pk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "fig = plt.figure(figsize=(20, 30))\n",
    "\n",
    "for i, pic in enumerate(sample_pictures):\n",
    "    img_path = complete_imgs_folder / pic\n",
    "    a = fig.add_subplot(np.ceil(len(sample_pictures)/float(ncols)), ncols, i + 1)\n",
    "    plt.imshow(matimg.imread(str(img_path)))\n",
    "    a.set_title(img_path.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pln.ggplot(pln.aes('date_in', 'price', group='factor(house_pk)'), data=data.loc[data['house_pk'].isin(sample_of_house_pk)])\n",
    " + pln.geom_line(pln.aes(color='factor(house_pk)'), alpha=0.8, size=1)\n",
    " + pln.labels.xlab('Date')\n",
    " + pln.labels.ylab('Price')\n",
    " + pln.labels.labs(color='House_pk')\n",
    " + pln.theme_bw()\n",
    " + pln.theme(axis_text_x=pln.element_text(rotation=90, hjust=0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop two parts from a mosaic of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in glob.glob(str(complete_imgs_folder / '*.png')):\n",
    "    img_path = pathlib.Path(img_path)\n",
    "    with w_Image(filename=img_path) as img:\n",
    "        # crop the complete image so that only top left image is retained\n",
    "        with img.clone() as i:\n",
    "            i.crop(width=235, height=290)\n",
    "            i.trim(fuzz=0)  # remove white boundary around the image\n",
    "            i.save(filename=str(img_path.parent.parent / 'aerial' / img_path.name))\n",
    "\n",
    "        # as above, but retain only the plan (Google Map view)    \n",
    "        with img.clone() as i:\n",
    "            i.crop(left=240, width=320, height=300)\n",
    "            i.trim(fuzz=0)\n",
    "            i.save(filename=str(img_path.parent.parent / 'plans' / img_path.name))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as torch_models\n",
    "import torchvision.transforms as torch_transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch_models.resnext50_32x4d(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = model._modules.get('fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations necessary for torchvision models\n",
    "normalize = torch_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "to_tensor = torch_transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(image_name):\n",
    "    \"\"\"\n",
    "    Source of the function: https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c\n",
    "    Modifications: mine\n",
    "    \"\"\"\n",
    "    img = Image.open(image_name).convert('RGB')\n",
    "    t_img = Variable(normalize(to_tensor(img)).unsqueeze(0))\n",
    "    my_embedding = torch.zeros(2048)\n",
    "    \n",
    "    def copy_data(m, i, o):\n",
    "        \"\"\"Let's capture the input that is passed to the last layer of the neural network.\"\"\"\n",
    "        my_embedding.copy_(i[0].squeeze().data)\n",
    "\n",
    "    h = embed_layer.register_forward_hook(copy_data)\n",
    "    model(t_img)\n",
    "    h.remove()\n",
    "\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths = [pathlib.Path(x) for x in glob.glob(str(complete_imgs_folder.parent / 'aerial' / '*.png'))]\n",
    "aerial_embeddings = []\n",
    "\n",
    "for img_path in images_paths:\n",
    "    embedding = get_vector(img_path)\n",
    "    aerial_embeddings.append(embedding)\n",
    "    \n",
    "aerial_embeddings = np.stack(aerial_embeddings, axis=0)    \n",
    "aerial_embeddings = pd.DataFrame(aerial_embeddings, index=[path.stem for path in images_paths], \n",
    "                                 columns=[f'X_{i}' for i in range(aerial_embeddings.shape[1])])\n",
    "aerial_embeddings.to_csv('aerial_embeddings.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths = [pathlib.Path(x) for x in glob.glob(str(complete_imgs_folder.parent / 'plans' / '*.png'))]\n",
    "plans_embeddings = []\n",
    "\n",
    "for img_path in images_paths:\n",
    "    embedding = get_vector(img_path)\n",
    "    plans_embeddings.append(embedding)\n",
    "    \n",
    "plans_embeddings = np.stack(plans_embeddings, axis=0)    \n",
    "plans_embeddings = pd.DataFrame(plans_embeddings, index=[path.stem for path in images_paths],\n",
    "                                columns=[f'X_{i}' for i in range(plans_embeddings.shape[1])])\n",
    "plans_embeddings.to_csv('plans_embeddings.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerial_embeddings = pd.read_csv('aerial_embeddings.csv', index_col=0).assign(type_='aerial')\n",
    "plans_embeddings = pd.read_csv('plans_embeddings.csv', index_col=0).assign(type_='plan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = pd.concat([aerial_embeddings, plans_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_dim_reduction = umap.UMAP(metric='cosine', n_neighbors=20, min_dist=0.25, random_state=random_state)\n",
    "umap_data = umap_dim_reduction.fit_transform(all_embeddings.loc[:, all_embeddings.columns.str.startswith('X')].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_data = pd.DataFrame(umap_data, index=all_embeddings.index, columns=['UMAP_1', 'UMAP_2']).assign(type_=all_embeddings['type_']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pln.ggplot(pln.aes(x='UMAP_1', y='UMAP_2', label='index', color='type_'), data=umap_data)\n",
    "  + pln.geom_text()\n",
    "  + pln.theme_bw()\n",
    "  + pln.theme(figure_size=(12, 8))\n",
    "  + pln.ggtitle('UMAP mapping of the two embedding types.')\n",
    "  + pln.labs(color='Type')\n",
    "#  + pln.xlim((5, 7.5))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create models exclusively on the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = {'max_depth': [8, 10, 12, 15]}  # change the grid from the last time - we do not have that many observations, so shallower trees are preferred\n",
    "\n",
    "rf_cv = GridSearchCV(rf_model, rf_grid, scoring='neg_mean_squared_error', \n",
    "                     cv=RepeatedKFold(n_splits=5, n_repeats=3, random_state=random_state), \n",
    "                     n_jobs=2, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_, embeddings in zip(('Aerial', 'Plan'), (aerial_embeddings, plans_embeddings)):\n",
    "    embed_features = embeddings.columns[embeddings.columns.str.startswith('X')]\n",
    "    embed_training_set = training_set[['house_pk', 'price']].groupby('house_pk').mean().merge(embeddings, left_on='house_pk', right_index=True, sort=False)\n",
    "    \n",
    "    print(f'{type_} embeddings; training dataset shape: {embed_training_set[embed_features].shape}')\n",
    "    \n",
    "    # unfortunetly, I had not sufficient RAM to train LightGBM on this dataset which is considerably wider\n",
    "    rf_cv.fit(embed_training_set[embed_features], embed_training_set['price'])\n",
    "    \n",
    "    pkl.dump(rf_cv, open(f'rf_model_{type_.lower()}_mean.pkl', 'wb'))\n",
    "    print(f'{type_} model stored.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions from the embeddings' models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, True, True, figsize=(20, 7))\n",
    "# the two lines below are for common labels for both plots\n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "\n",
    "for i, (type_, embeddings) in enumerate(zip(('aerial', 'plan'), (aerial_embeddings, plans_embeddings))):\n",
    "    embed_features = embeddings.columns[embeddings.columns.str.startswith('X')]\n",
    "    embed_validation_set = training_set[['house_pk', 'price']].groupby('house_pk').mean().merge(embeddings, left_on='house_pk', right_index=True, sort=False)\n",
    "    \n",
    "    embed_model = pkl.load(open(f'rf_model_{type_.lower()}_mean.pkl', 'rb'))\n",
    "    \n",
    "    embed_model_predictions = embed_model.predict(embed_validation_set[embed_features])\n",
    "    ax[i].plot(embed_validation_set['price'], embed_model_predictions, color='green', marker='x', linewidth=0)\n",
    "    ax[i].set_title(f'Random forest predictions {type_} embedding \\n MSE: {np.round(mean_squared_error(embed_validation_set[\"price\"], embed_model_predictions), 5)}',\n",
    "                    fontdict={'fontsize': 20, 'fontweight': 'bold'})\n",
    "    ax[i].plot([0,1],[0,1], transform=ax[i].transAxes)  # add y=x line\n",
    "\n",
    "plt.xlabel('log10(Real price)')\n",
    "plt.ylabel('log10(Predicted price)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create second order models for houses with pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training and validation observations where the picture is provided\n",
    "training_set_with_pictures = training_set.merge(aerial_embeddings, left_on='house_pk', right_index=True, sort=False)\n",
    "validation_set_with_pictures = validation_set.merge(aerial_embeddings, left_on='house_pk', right_index=True, sort=False)\n",
    "\n",
    "# get all models we want to use\n",
    "lgbm_cv = pkl.load(open('lgbm_model.pkl', 'rb'))\n",
    "rf_aerial_cv = pkl.load(open('rf_model_aerial_mean.pkl', 'rb'))\n",
    "rf_plan_cv = pkl.load(open('rf_model_plan_mean.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from the three models that we trained\n",
    "lgbm_embed_predictions = lgbm_cv.predict(training_set_with_pictures[X_features])\n",
    "rf_aerial_predictions = rf_aerial_cv.predict(training_set_with_pictures[aerial_embeddings.columns[aerial_embeddings.columns.str.startswith('X')]])\n",
    "rf_plan_predictions = rf_plan_cv.predict(training_set.merge(plans_embeddings, left_on='house_pk', right_index=True, sort=False)[plans_embeddings.columns[plans_embeddings.columns.str.startswith('X')]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_predictions = pd.DataFrame({\n",
    "    'LGBM': lgbm_cv.predict(training_set_with_pictures[X_features]), \n",
    "    'Aerial': rf_aerial_cv.predict(training_set_with_pictures[aerial_embeddings.columns[aerial_embeddings.columns.str.startswith('X')]]), \n",
    "    'Plan': rf_plan_cv.predict(training_set.merge(plans_embeddings, left_on='house_pk', right_index=True, sort=False)[plans_embeddings.columns[plans_embeddings.columns.str.startswith('X')]]), \n",
    "    'price': training_set_with_pictures['price']\n",
    "})\n",
    "\n",
    "all_validation_predictions = pd.DataFrame({\n",
    "    'LGBM': lgbm_cv.predict(validation_set_with_pictures[X_features]), \n",
    "    'Aerial': rf_aerial_cv.predict(validation_set_with_pictures[aerial_embeddings.columns[aerial_embeddings.columns.str.startswith('X')]]), \n",
    "    'Plan': rf_plan_cv.predict(validation_set.merge(plans_embeddings, left_on='house_pk', right_index=True, sort=False)[plans_embeddings.columns[plans_embeddings.columns.str.startswith('X')]]), \n",
    "    'price': validation_set_with_pictures['price']    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_model = ElasticNet(random_state=random_state)\n",
    "\n",
    "elasticnet_grid = {\n",
    "             'alpha': Real(0, 1, 'uniform'),\n",
    "             'l1_ratio': Real(0, 1, 'uniform'), \n",
    "            }\n",
    "\n",
    "elasticnet_cv = BayesSearchCV(elasticnet_model, elasticnet_grid, n_iter=25, scoring='neg_mean_squared_error', refit=True, n_points=1,\n",
    "                              cv=RepeatedKFold(n_splits=5, n_repeats=3, random_state=random_state),\n",
    "                              n_jobs=3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_cv.fit(all_training_predictions[['LGBM', 'Aerial', 'Plan']], all_training_predictions['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_cv.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, True, True, figsize=(20, 14))\n",
    "# the two lines below are for common labels for both plots\n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "\n",
    "ax[0][0].plot(all_validation_predictions['price'], elasticnet_cv.predict(all_validation_predictions[['LGBM', 'Aerial', 'Plan']]), \n",
    "           color='green', marker='x', linewidth=0)\n",
    "ax[0][0].set_title(f'Predictions from second order model - MSE: {np.round(mean_squared_error(all_validation_predictions[\"price\"], elasticnet_cv.predict(all_validation_predictions[[\"LGBM\", \"Aerial\", \"Plan\"]])), 5)}',\n",
    "                fontdict={'fontsize': 20, 'fontweight': 'bold'})\n",
    "\n",
    "ax[0][1].plot(validation_set_with_pictures['price'], lgbm_cv.predict(validation_set_with_pictures[X_features]), color='blue', marker='x', linewidth=0)\n",
    "ax[0][1].set_title(f'LightGBM predictions - MSE: {np.round(mean_squared_error(validation_set_with_pictures[\"price\"], lgbm_cv.predict(validation_set_with_pictures[X_features])), 5)}',\n",
    "                fontdict={'fontsize': 20, 'fontweight': 'bold'})\n",
    "\n",
    "ax[1][0].plot(elasticnet_cv.predict(all_validation_predictions[['LGBM', 'Aerial', 'Plan']]), \n",
    "              lgbm_cv.predict(validation_set_with_pictures[X_features]), color='red', marker='o', linewidth=0)\n",
    "ax[1][0].set_title(f'Comparison of LightGBM and second-order model predictions \\n MSE: {np.round(mean_squared_error(elasticnet_cv.predict(all_validation_predictions[[\"LGBM\", \"Aerial\", \"Plan\"]]), lgbm_cv.predict(validation_set_with_pictures[X_features])), 5)}', \n",
    "                   fontdict={'fontsize': 16, 'fontweight': 'bold'})\n",
    "\n",
    "ax.flat[-1].set_visible(False)\n",
    "\n",
    "plt.xlabel('log10(Real price)')\n",
    "plt.ylabel('log10(Predicted price)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
